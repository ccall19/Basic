{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf5309b",
   "metadata": {},
   "source": [
    "[`原文档地址`](https://hyper.ai/cn/tutorials/46989)\n",
    "# 使用 PyTorch 实现深度学习\n",
    "\n",
    "本教程的目标：\n",
    "\n",
    "- 理解在 PyTorch 中如何使用张量和构建神经网络。\n",
    "\n",
    "- 训练一个小型神经网络对图像进行分类\n",
    "\n",
    "### 目录：\n",
    "- 张量\n",
    "- 自动微分\n",
    "- 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b97b5a",
   "metadata": {},
   "source": [
    "## 一 、张量\n",
    "张量是一种特殊的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量对模型的输入和输出以及模型的参数进行编码。\n",
    "\n",
    "张量类似于 NumPy 的 ndarray 对象，只是张量可以在 GPU 或其他专用硬件上运行以加速计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ef1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea5be2",
   "metadata": {},
   "source": [
    "### Z1 张量的初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611e7ff",
   "metadata": {},
   "source": [
    "### 1 从数据创建\n",
    "张量可以直接从数据中创建。PyTorch 将自动推断数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799c5bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c44877",
   "metadata": {},
   "source": [
    "### 2 从 NumPy 数组创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1b4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44802f",
   "metadata": {},
   "source": [
    "### 3 从另一个张量创建\n",
    "新张量将默认保留旧张量的属性（shape, datatype）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0590d974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.3243, 0.2383],\n",
      "        [0.7441, 0.6572]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # 保留 x_data 的属性，创建元素值全为 1 的张量\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # 重写 x_data 的 datatype，创建元素值为随机数的张量\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c074b7",
   "metadata": {},
   "source": [
    "### 4 使用随机值或常量值创建\n",
    "shape 是由张量维度组成的元组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbef1275",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape) #张量元素值为随机数\n",
    "ones_tensor = torch.ones(shape) #张量元素值全为 1\n",
    "zeros_tensor = torch.zeros(shape) #张量元素值全为 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c683af8",
   "metadata": {},
   "source": [
    "### Z2 张量属性\n",
    "张量属性包括张量的形状(shape)、数据类型(datatype)和存储设备(device)。       \n",
    "tensor.shape | tensor.dtype | tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4957d4a",
   "metadata": {},
   "source": [
    "### Z3 张量运算\n",
    "包括转置、索引、切片、数学运算、线性代数、随机抽样等。详细参看 [`官方文档`](https://docs.pytorch.org/docs/stable/torch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "609a3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9a4c5",
   "metadata": {},
   "source": [
    "### 1 索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8101efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f7aac",
   "metadata": {},
   "source": [
    "### 2 连接\n",
    "torch.cat 可以沿给定的维度方向连接输入张量。torch.stack 是另一种张量连接方式，它沿一个新维度连接输入张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19f392bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) torch.Size([4, 12])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1, t1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce7c8f",
   "metadata": {},
   "source": [
    "### 3 张量乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a498f9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "\n",
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#计算对应元素的乘积\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "#替代语法:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\\n\")\n",
    "\n",
    "#两个张量之间的矩阵乘法\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "#替代语法:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8214c6",
   "metadata": {},
   "source": [
    "### 4 原地操作\n",
    "带有 _ 后缀的操作将在原地操作，即直接在原本的内存上改变值。例如：x.copy_(y)，x.t_() 将直接更改x。       \n",
    "原地操作可以节省内存，但在计算梯度时由于会立即丢失历史记录而导致问题，因此，计算梯度时不鼓励使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4303d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[2., 1., 2., 2.],\n",
      "        [2., 1., 2., 2.],\n",
      "        [2., 1., 2., 2.],\n",
      "        [2., 1., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(1)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1708bd1b",
   "metadata": {},
   "source": [
    "## 二 、自动微分\n",
    "torch.autograd 是 PyTorch 的自动微分技术，用于帮助神经网络训练。        \n",
    "神经网络(Neural Networks,NNs)是对输入数据操作的嵌套函数的集合。这些函数由参数（包括权重和偏差）确定，这些参数在 PyTorch 中都存储为张量形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa1e8e",
   "metadata": {},
   "source": [
    "### 1 PyTorch中的用法\n",
    "从 torchvision 加载预训练的 resnet18 模型，同时使用一个随机数据张量来表示一个具有 3 个通道的高度和宽度均为 64 的图像，其相应的标签初始化为随机值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50630d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lllsh/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/lllsh/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a924815",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) #前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dfc33",
   "metadata": {},
   "source": [
    "当我们调用误差张量上的 .backward() 时，反向传播启动，torch.autograd 将自动计算模型里每个参数的梯度并将其存储在参数的 .grad 属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b73edbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e861157",
   "metadata": {},
   "source": [
    "接下来，我们使用优化器优化参数，在示例中，我们使用 SGD，并设置学习率为 0.01，动量为 0.9。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33c3e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa8fe0e",
   "metadata": {},
   "source": [
    "最后，我们调用 .step() 来启动梯度下降算法，优化器将根据参数 .grad 中存储的梯度来优化每个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "642efba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205492d",
   "metadata": {},
   "source": [
    "## 三、神经网络\n",
    "更新网络参数，通常使用的更新规则为：参数 = 参数 - 学习率 * 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061bb50",
   "metadata": {},
   "source": [
    "### 1 定义网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c1afb",
   "metadata": {},
   "source": [
    "torch.nn.functional (通常缩写为 F) 是 PyTorch 中的一个模块，提供神经网络的函数式 API。它包含各种操作函数，如激活函数、池化、损失函数等，这些函数不需要创建类实例，直接调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d2f6601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 个输入通道，6 个输出通道，5 x 5 平方卷积核\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #仿射运算：y=Wx+b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5 * 5 来自图像尺寸\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #使用（2，2）窗口最大池化\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        #如果大小为正方形，则可以使用单个数字指定\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) #展平除批量维度以外的所有维度\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873dc164",
   "metadata": {},
   "source": [
    "只需定义前向函数，反向函数（计算梯度）将使用 autograd 自动定义。我们可以在正向函数中使用任何张量运算。\n",
    "\n",
    "模型的可学习参数由 net.parameters() 返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ca873cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  #conv1权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3ea79",
   "metadata": {},
   "source": [
    "### 2 损失函数与优化器\n",
    "\n",
    "当我们使用神经网络时，我们会需要使用各种不同的更新规则，如 SGD、Nesterov SGD、Adam、RMSProp 等。为了实现这一点，PyTorch 构建了一个小软件包：torch.optim，它实现了所有这些方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#创建优化器\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "target = torch.randn(10)  #假设的目标\n",
    "\n",
    "#在训练循环中：\n",
    "optimizer.zero_grad()   #初始化梯度\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    #更新权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761fe1e",
   "metadata": {},
   "source": [
    "一句话描述训练过程：只需在数据迭代器上循环将数据正向输入到网络并反向传播，更新权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd88119",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  #在数据集上循环多次\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 1):\n",
    "        #获取输入；数据是[输入、标签]列表\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #初始化参数梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #前向传播、反向传播、更新权重\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #打印统计数据\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 199:    #每 2000个批量打印一次\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ce743",
   "metadata": {},
   "source": [
    "保存训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa284e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
