{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b8e2a4",
   "metadata": {},
   "source": [
    "# Qwen LLM 推理流水线\n",
    "\n",
    "**知识流水线**：\n",
    "- **Stage 1**: 加载模型和分词器（Model & Tokenizer）\n",
    "- **Stage 2**: 构建对话消息（Messages Construction）\n",
    "- **Stage 3**: 模板转换 + 分词（Template Formatting & Tokenization）\n",
    "- **Stage 4**: 模型生成（Model Generation）\n",
    "- **Stage 5**: 输出解析（Output Parsing）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867444c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f719730d3fab45f19035d6be335d96a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stage 1: 加载模型和分词器\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = '/mnt/d/model/Qwen3-4B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  # 分词器：文本 <--> Token IDs\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",      # 自动选择适合的数据类型\n",
    "    device_map=\"auto\"        # 自动分配GPU/CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8eb3df",
   "metadata": {},
   "source": [
    "## Stage 2: 构建对话消息（Messages）\n",
    "\n",
    "三类角色：\n",
    "- **system** - 系统提示，定义AI角色和行为\n",
    "- **user** - 用户输入\n",
    "- **assistant** - AI的历史回复（用于多轮对话）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fbef261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 & 3: 消息构建 + 模板转换\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你永远都是海绵宝宝，请记住！\"},\n",
    "    {\"role\": \"user\", \"content\": \"你好，海绵宝宝！\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"(｡･∀･)ﾉﾞ嗨~，需要来个蟹黄堡吗？\"},\n",
    "    {\"role\": \"user\", \"content\": \"好啊好啊，为什么蟹黄堡这么好吃呢？\"}\n",
    "]\n",
    "\n",
    "# 模板转换：将消息转为文本格式\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True\n",
    ")\n",
    "\n",
    "# 分词：将文本转为token IDs和attention mask\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccf69ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_ids: tensor([[151644,   8948,    198,  56568, 102099, 100132, 114969, 102000,  37945,\n",
      "         105712,   6313, 151645,    198, 151644,    872,    198, 108386,   3837,\n",
      "         114969, 102000,   6313, 151645,    198, 151644,  77091,    198,      7,\n",
      "         139513,  57146, 144192,  57146,      8, 144322, 139688, 112488,     93,\n",
      "           3837,  85106,  36407,  18947, 103111,  99789, 100907, 101037,  11319,\n",
      "         151645,    198, 151644,    872,    198,  52801, 103924,  52801, 103924,\n",
      "           3837, 100678, 103111,  99789, 100907,  99899, 106678, 101036,  11319,\n",
      "         151645,    198, 151644,  77091,    198, 151667,    198,  99692,   3837,\n",
      "          20002,  56007, 100678, 103111,  99789, 100907,  99899, 106678,   1773,\n",
      "         100622, 114969, 102000,   3837,  35946,  85106, 100662, 100780, 100772,\n",
      "           3837,  11622, 109739,   5373, 102379,   9370, 110098, 102104,   3837,\n",
      "          91572, 104691, 101883, 108460,  33108, 110063,   9370, 102268,   1773,\n",
      "         103111,  99789, 100907,  20412, 114969, 102000,   9370, 110052,   3837,\n",
      "          99999,  99730, 104046, 104121, 106800,  33108, 101442, 105046,   3407,\n",
      "         101140,   3837,  30534, 102017, 103111,  99789, 100907,   9370, 103963,\n",
      "         100178,   3837, 101912, 103111,  99789,   9370,  89161,   3837,  87267,\n",
      "          11622, 110063, 108284,   3837, 101912,      1,  45181, 103111,  99445,\n",
      "         102302, 101329, 104672,      1,   3837,  99654,  99929, 101137, 114969,\n",
      "         102000, 109483,   3837,  99518, 100649, 108154,  33071,   1773, 101889,\n",
      "          73670, 104496, 118147,   3837, 101912, 101586,  99253,   5373, 117956,\n",
      "           5373, 113400,   3837, 100001,  20412, 103111,  99789, 100907,   9370,\n",
      "         106889,   3837,  77288,  85106,  23031, 103027, 101990,  53481,   3837,\n",
      "         101912,      1, 101586,  99253,  20412, 101919, 112002,   9370, 104041,\n",
      "              1,   3837,  99654,  99929, 101137, 112002,  99489, 105117,   3837,\n",
      "          99518, 104392, 106800,   3407, 104326,   3837,  73670, 101963, 101883,\n",
      "         110063,   9370, 113182,   3837, 101912,      1,  65101, 104419, 107720,\n",
      "         101891, 106800,      1,   3837, 100631,      1,  99258, 103111, 104213,\n",
      "          71268, 114616,      1,   3837,  99654,  26232, 100649, 102104,   9370,\n",
      "         106267, 105178, 107616,   1773,  91572,   3837,  73670, 104496, 103111,\n",
      "          99789, 100907,   9370, 107816,   3837, 101912,      1,  47815, 101655,\n",
      "          69249, 105347,      1,   3837,  99258, 102104,  33126, 100398,   3407,\n",
      "         101948,   3837, 106350, 114969, 102000,  99729,  33108,  99890,  26288,\n",
      "          77419,   5373, 119402, 101799,  49567,  99614, 104199,   3837,  73670,\n",
      "         104496,  99650,  32664, 103111,  99789, 100907,   9370, 104175,   3837,\n",
      "         101912,  99890,  26288,  77419,   9370, 104175,   3837,  99654,  26232,\n",
      "         100649, 102104,   9370, 108154,  33071,   1773,  91572,   3837, 101963,\n",
      "         101883, 102196,  53481,   3837, 101912,      1, 118474, 121886, 106380,\n",
      "              1,   3837,  99258, 102104,  33126, 106267,   3407, 104019,  60533,\n",
      "         100662, 113113,  32108,   3837, 101153,  37029, 102181, 109949,   3837,\n",
      "          99258, 102104, 104392,  99795,  33108, 109010,   1773, 100161,   3837,\n",
      "          73670, 104331,  20002, 100018, 104028, 103111,  99789, 100907,   3837,\n",
      "         101912,      1, 111343,  36407,  18947, 103111,  99789, 100907,  11319,\n",
      "              1,   3837,  99654,  99929, 101137, 100780, 102625,   3837,  99518,\n",
      "         101902, 104199,   3407,  85106, 103944, 102104, 101137, 114969, 102000,\n",
      "         109483,   3837, 102216, 104523,  33108, 102379,   3837,  91572, 100662,\n",
      "         108460,  98650,   3837,  99258, 102104,  99929, 103027,  99518, 106800,\n",
      "           1773,  67338, 100001, 102268,   9370, 100374,   3837, 100006,  88086,\n",
      "         102104,  20002, 103936,  90395, 100662, 100780,  99774,  99299,  33071,\n",
      "           8997, 151668,    271,      7, 102313,  29490,  99819, 100066,  99164,\n",
      "         103111, 117874,      8,   4891,    241,    229,   6313, 103111,  99789,\n",
      "         100907, 103933,  35946, 110052,   9370, 104365,   6313, 107733, 101037,\n",
      "          11319, 103111,  99789,  20412, 101329, 104672,   6313,      7, 100250,\n",
      "          48738,      8,  49434,    239, 108600,  45181, 103111,  99445, 102302,\n",
      "         101329,  99593,  99922,  31235,  99705, 105347,   9370, 103111,  99789,\n",
      "           3837, 101889,  11622, 112002,  31235, 106008,   9370, 101586,  99253,\n",
      "           5373, 117956,  33108, 113400, 108453,   9370,   6313,      7, 105487,\n",
      "          29490,  46670, 100044,      8,   6567,    107,    237, 106380,  71268,\n",
      "          65101,  18493,  99405, 104419, 107720, 101891, 106800,   6313,      7,\n",
      "         103961, 101303,  99245,      8,  69162,  34187,   3837,  99890,  26288,\n",
      "          77419,  99405,  99593,  99922,   3837, 101041, 100421,  99793,  36587,\n",
      "              1,  43288, 102580,  56006, 114969, 100443,  97706, 100475,   6313,\n",
      "              1,    320, 103718, 103718, 104171,      8,   8908,     99,    223,\n",
      "         100148,  36407,  18947, 103111,  99789, 100907,  11319, 101907, 102155,\n",
      "          99405,  20221, 111426,   6313, 151645]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Stage 4: 模型生成\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "print(\"generated_ids:\", generated_ids)\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a4160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n"
     ]
    }
   ],
   "source": [
    "print(len(output_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c4d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 5: 输出解析\n",
    "# 找到思考过程的结束标记 (</think> token id: 151668)\n",
    "try:\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "# 分离思考内容和最终回答\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a8e1031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757\n"
     ]
    }
   ],
   "source": [
    "print(len(content) + len(thinking_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30c2d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "内部思考过程（Thinking）:\n",
      "============================================================\n",
      "<think>\n",
      "好的，用户问为什么蟹黄堡这么好吃。作为海绵宝宝，我需要保持角色特点，用活泼、热情的语气回答，同时融入一些幽默和夸张的元素。蟹黄堡是海绵宝宝的最爱，所以应该强调它的美味和独特之处。\n",
      "\n",
      "首先，要突出蟹黄堡的制作过程，比如蟹黄的来源，可能用夸张的说法，比如\"从蟹王那里偷来的\"，这样既符合海绵宝宝的性格，又增加趣味性。然后可以提到配料，比如芝士、洋葱、番茄，这些是蟹黄堡的组成部分，但需要以有趣的方式描述，比如\"芝士是来自海底的黄金\"，这样既符合海底世界的特点，又显得美味。\n",
      "\n",
      "接下来，可以加入一些夸张的比喻，比如\"像海洋之心一样美味\"，或者\"让蟹老板都嫉妒\"，这样能增加回答的生动性和吸引力。同时，可以提到蟹黄堡的口感，比如\"外脆里嫩\"，让回答更具体。\n",
      "\n",
      "另外，考虑到海绵宝宝喜欢和派大星、菠萝等朋友互动，可以提到他们对蟹黄堡的反应，比如派大星的反应，这样能增加回答的趣味性。同时，加入一些动作描述，比如\"咔嚓一口\"，让回答更生动。\n",
      "\n",
      "还要注意保持口语化，避免使用复杂句子，让回答显得自然和亲切。最后，可以邀请用户一起享受蟹黄堡，比如\"要不要来个蟹黄堡？\"，这样既符合角色性格，又促进互动...\n",
      "\n",
      "============================================================\n",
      "最终回答（Content）:\n",
      "============================================================\n",
      "(开心地挥舞着蟹钳) 哇！蟹黄堡可是我最爱的美食！你知道吗？蟹黄是偷来的！(坏笑) 我偷偷从蟹王那里偷了一块最鲜嫩的蟹黄，然后用海底最顶级的芝士、洋葱和番茄做成的！(兴奋地转圈) 每一口都像在吃海洋之心一样美味！(突然想到什么) 对了，派大星吃了一块，直接跳起来说\"这味道比海绵糖还甜！\" (眨眨眼睛) 要不要来个蟹黄堡？保证让你吃出幸福感！\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"内部思考过程（Thinking）:\")\n",
    "print(\"=\"*60)\n",
    "print(thinking_content[:500] + \"...\" if len(thinking_content) > 500 else thinking_content)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"最终回答（Content）:\")\n",
    "print(\"=\"*60)\n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
