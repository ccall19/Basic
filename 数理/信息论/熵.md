[Wiki信息熵](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))

# 0、Shannon 熵
信息论中，熵是每条消息中包含的信息的平均量，这里的消息代表来自分布或数据流中的事件、样本或特征。。熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。

当取自有限的样本时，熵的公式可以表示为：

$$H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i$$

其中 $p_i$ 是事件 $i$ 出现的概率，$\log_2$ 是以 2 为底的对数（信息量单位为比特）。

对数所使用的底，通常是2，自然常数e，或是10。当b = 2，熵的单位是bit；当b = e，熵的单位是nat；而当b = 10,熵的单位是Hart。

# 1、Patch 

由于 Transformer 的自注意力机制擅长捕捉序列中元素间的长距离依赖关系，且预训练大模型已在海量数据上学到了通用的序列建模能力，我的时间序列模型采用预训练大模型的 Transformer 模块作为主干网络。由此自然想到类比大模型 NLP 任务中对于 Token 的处理。

每一个Patch 在神经网络层面 等同于一个 Token，在自然语言任务中，一个Token往往是一个带有确切的语义字或词或是词缀，即都带有一定的信息量，因此想到以一个Patch 也应该带有足够信息量。

虽然时间序列划分后的 Patch 依然是一段连续的 信息，但是由于之后会将一个Patch视作一个整体投影到嵌入空间中，所以一个Patch 也可以看作是一个离散的点，作为时间序列的一个状态存在。 时间序列千变万化，不同的滑动窗口长度也会带来完全不同的Patch 形式，所以时间序列任务不像自然语言任务一样，可以针对定量的 token 预训练一个嵌入矩阵出来。

针对Patch，一个问题是，怎样的一组序列 适合整体表示 一个状态？

可以计算的两个熵值：完整时间序列的熵、单个Patch的熵。

理论上，序列变长，可能出现相同的 `模式` 致熵减，也会出现新`模式`致熵增。

标准熵公式中采用求和运算，但是当某个序列多次出现小概率事件时，计算该序列的熵依旧沿用线性求和是否不足以表现这种小概率事件的真实混乱程度以及信息含量？

需要对Patch 内重复的模式做出奖励或惩罚，罕见的模式出现，对于熵的影响不应该只是简单的 和 运算，同样的，普通模式多次出现也不应该只是简单的 和 运算。

因此，需要一种运算法则，两个相同的数字 计算，大者越大，小者越小，由于是概率值的计算，都是小于1 的数。

在此注意到 `Tsallis熵（非广延熵）` ：

### 什么是"广延性"？
-----
经典热力学中的广延量与系统大小成正比，例如：质量、体积、能量、熵。

两杯水合并 → 质量加倍、体积加倍

熵的广延性（经典情况）:对于两个统计独立的子系统A和B：经典玻尔兹曼-吉布斯熵满足：

$$S(A+B) = S(A) + S(B)$$ 

这就是广延性：总熵 = 各部分熵之和

### Tsallis熵的非广延性
------
对于两个独立子系统，Tsallis熵满足：

$$S_q(A+B) = S_q(A) + S_q(B) + (1-q)/k · S_q(A) · S_q(B)$$

出现了一个交叉项，总熵 ≠ 简单求和

#### 参数q的作用
- q = 1：交叉项消失，退化为经典广延熵
- q > 1：交叉项为负，亚广延（Sub-extensive）
$$S_q(A+B) < S_q(A) + S_q(B)$$
- q < 1：交叉项为正，超广延（Super-extensive）
$$S_q(A+B) > S_q(A) + S_q(B)$$


### Tsallis熵

$$S_q = \frac{k}{q-1}\left(1 - \sum_{i=1}^{n} p_i^{\,q}\right)$$

其中 $q$ 为非广延参数（$q \in \mathbb{R},\, q \neq 1$），$k$ 为正常数（通常取1），$p_i$ 为第 $i$ 个状态的概率。当 $q \to 1$ 时，Tsallis熵退化为标准 Boltzmann-Gibbs 熵（即Shannon熵的连续形式）。

### 受 热力学 启发 为 香农熵 添加 非广延项
------
$$H(X) = -\sum_{i=1}^{n} [p_i \log_2 p_i + \theta \cdot \alpha_i \cdot (1 - p_i)]$$

其中：
- $p_i$：模式 $i$ 的先验概率，由外部训练数据的二阶马尔可夫链转移矩阵得到
- $\alpha_i$：模式 $i$ 在当前 Patch 内的出现频率，$\alpha_i = c_i / N$，其中 $c_i$ 为出现次数，$N$ 为当前 Patch 内总观测样本数。归一化后不同 Patch 长度之间可直接比较
- $\theta$：超参数，控制非广延修正项的整体权重
- $(1 - p_i)$：罕见度因子，$p_i$ 越小该值越大，使罕见模式获得更大的修正加成

**修正项 $\theta \cdot \alpha_i \cdot (1-p_i)$ 的直觉：**
- 罕见模式（$p_i$ 小）频繁出现（$\alpha_i$ 大）→ 修正项大，熵显著增加
- 常见模式（$p_i$ 大）正常出现 → 修正项小，接近标准 Shannon 熵
- 当 $\theta = 0$ 时，退化为标准 Shannon 熵

# 2、Step Size

Patch 解决了"多少数据构成一个状态"的问题，Step 则决定了相邻 Patch 之间的重叠区大小——即用多大的共享片段来表达上下 Patch 的相关性。

### NLP 类比：为什么时间序列需要重叠

延续 Patch 部分的 NLP 类比，可以进一步思考 Step 的角色：

| NLP | 时间序列 |
|-----|---------|
| Token | Patch（信息承载单元） |
| 相邻词的语义关系（"他的 苹果"） | Step 控制的 Patch 重叠区 |
| 分词粒度 | Patch 长度 |
| 词序/语法 | Step 决定的相邻 Patch 关联 |

**关键差异在于：**

在 NLP 中，预训练的 Transformer 在海量文本上训练过，已经学会了 Token 之间的上下文关系——词汇表有限、语法规则确定，模型对"他的"后面大概率跟名词这件事已有先验知识。因此 NLP 任务中 Transformer 自身就能建立上下文关联。

但时间序列完全不同：
- 时间序列的状态空间**理论上无限**——不存在固定词汇表，Patch 的形态千变万化
- 预训练模块**不可能提前学习**所有时间序列 Patch 之间的关系
- 即使 Transformer 的自注意力机制擅长捕捉上下文关系，这里**不再是 NLP 任务**，不能假设它对时序 Patch 间的依赖关系有足够的先验认知

**因此，我们需要主动设置重叠区，将相邻 Patch 的共享数据点作为显式的"上下文线索"提供给 Transformer，向其提供相邻 Patch 之间的相互关系。** 重叠区越大，提供的关联信息越多，但冗余也随之增加。

### 信息论视角：重叠区大小的选择准则

重叠区大小 = patch_len - step。用条件熵衡量重叠带来的关联强度：

$$\Delta H(s) = H'(P_{t} \cup P_{t+s}) - H'(P_t)$$

- step 很小 → $\Delta H$ 很小，冗余严重，浪费 Attention 容量
- step = patch_len → 零重叠，Attention 完全依赖自身发现关联（对时间序列不利）
- **$\Delta H(s)$ 边际递减的拐点 → 最优 step**：再增加重叠，新信息增益不再值得额外的计算开销

**step = patch_len / 2**（半重叠），在信息增益和计算效率之间取平衡，是 PatchTST 等时间序列 Transformer 的常用做法。

......
